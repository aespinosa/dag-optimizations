\documentclass{article}

\usepackage[binary]{SIunits}

\title{Comparison of data-aware scheduling and task clustering}
\author{Allan Espinosa}

\begin{document}

\section{Introduction} % (fold)
\label{sec:introduction}
We compare workflow optimization techniques like task clustering to a data-aware
scheduler.  Both approaches seek to reduce the overall makespan of a DAG
workflow by reducing the amount of data movement in between jobs.

% section introduction (end)

\section{Related work} % (fold)
\label{sec:related_work}

Various work on task clustering.  Some people also call this process graph
partitioning.  Pegasus \cite{Singh2008} team used level-based and user-specified
task clustering schemes.  Their system assumes that the users know how to best
partition their tasks well.  Tanaka and Tatebe \cite{Tanaka2012} used
multi-constraint graph partitioning algorithms.  This technique was
traditionally used for the domain decomposition of finite-element meshes.

Data-aware scheduling people.  Thain et.al.\ \cite{Thain2003} defined datasets
into pipeline and batch on how they are used in grid workloads.  

% section related_work (end)

\section{Experiment setup} % (fold)
\label{sec:experiment_setup}

With the two setups in the next sub section we run two scheduling schemes:
data-aware scheduling defined by me.  And a DAG-optimized version of the
pipeline ensemble.

In the first stage, we schedule jobs in a first-come-first-serve fashion.  Then
for the second stage, we schedule jobs on where the first job executed.  If
there are no available processors, like the CPU still being busy, schedule it to
the next processor and move the data.  We use the  ``max-cache-hit'' dispatch
policy describe in \cite{Raicu2008a}.

For our resources, we have a central storage where the input data will come from
and two compute clusters each with its own file system.  The two clusters are
connected by a backbone link with the central storage.

\subsection{Pure pipeline} % (fold)
\label{sub:pure_pipeline}

% subsection Pure pipeline (end)

We create an $n$-wide set of independent pipeline of tasks.  Each pipeline has
two job.  The first jobs will produce data that is to be consumed by the next
process.  In the workflow, the first stage of the pipeline gets data from a
single source and the second stage persists data to the same storage area.

Each stage has a runtime of $\mu$~\second according to a normal distribution.
Same goes to each second stage.  Uniform distribution is also used on the input
file, intermediate data and output file.

\subsection{Share data only} % (fold)
\label{sub:shared_data_only}

The other setup is where the input for each job of a stage is the same for that
stage.  Some in the literature call this ``common data''.

% subsection shared_data_only (end)

% section experiment_setup (end)

\end{document}
