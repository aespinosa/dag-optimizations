Automatically generated by Mendeley 1.5.2
Any changes to this file will be lost if it is regenerated by Mendeley.

@inproceedings{Thain2003,
author = {Thain, Douglas and Bent, John and Arpaci-Dusseau, Andrea and Arpaci-Dusseau, Remzi and Livny, Miron},
booktitle = {High Performance Distributed Computing},
doi = {10.1109/HPDC.2003.1210025},
file = {:home/aespinosa/Documents/ResearchPapers/Thain et al. - 2003 - Pipeline and batch sharing in grid workloads.pdf:pdf},
isbn = {0769519652},
pages = {152--161},
publisher = {IEEE Comput. Soc},
title = {{Pipeline and batch sharing in grid workloads}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1210025},
year = {2003}
}
@techreport{Raicu2008a,
abstract = {Data intensive applications often involve the analysis of large datasets that require large amounts of compute and storage resources. While dedicated compute and/or storage farms offer good task/data throughput, they suffer low resource utilization problem under varying workloads conditions. If we instead move such data to distributed computing resources, then we incur expensive data transfer cost. In this paper, we propose a data diffusion approach that combines dynamic resource provisioning, on-demand data replication and caching, and data locality-aware scheduling to achieve improved resource efficiency under varying workloads. We define an abstract "data diffusion model" that takes into consideration the workload characteristics, data accessing cost, application throughput and resource utilization; we validate the model using a real-world large-scale astronomy application. Our results show that data diffusion can increase the performance index by as much as 34X, and improve application response time by over 506X, while achieving near-optimal throughputs and execution times.},
archivePrefix = {arXiv},
arxivId = {0808.3535},
author = {Raicu, Ioan and Zhao, Yong and Foster, Ian and Szalay, Alex},
booktitle = {Design},
eprint = {0808.3535},
file = {:home/aespinosa/Documents/ResearchPapers/Raicu et al. - 2008 - Data Diffusion Dynamic Resource Provision and Data-Aware Scheduling for Data Intensive Applications.pdf:pdf},
institution = {The University of Chicago},
keywords = {data,data aware,data caching,data intensive applications,data management,data-aware,data-intensive applications,diffusion,dynamic resource provisioning,falkon,grid,scheduling},
month = aug,
pages = {16},
title = {{Data Diffusion: Dynamic Resource Provision and Data-Aware Scheduling for Data Intensive Applications}},
url = {http://arxiv.org/abs/0808.3535},
year = {2008}
}
@inproceedings{Singh2008,
abstract = {Many scientific workflows are composed of fine computational granularity tasks, yet they are composed of thousands of them and are data intensive in nature, thus requiring resources such as the TeraGrid to execute efficiently. In order to improve the performance of such applications, we often employ task clustering techniques to increase the computational granularity of workflow tasks. The goal is to minimize the completion time of the workflow by reducing the impact of queue wait times. In this paper, we examine the performance impact of the clustering techniques using the Pegasus workflow management system. Experiments performed using an astronomy workflow on the NCSA TeraGrid cluster show that clustering can achieve a significant reduction in the workflow completion time (up to 97\%).},
author = {Singh, Gurmeet and Su, Mei-Hui and Vahi, Karan and Deelman, Ewa and Berriman, Bruce and Good, John and Katz, Daniel and Mehta, Gaurang},
booktitle = {Mardis Gras Conference},
doi = {10.1145/1341811.1341822},
file = {:home/aespinosa/Documents/ResearchPapers/Singh et al. - 2008 - Workflow task clustering for best effort systems with Pegasus.pdf:pdf},
isbn = {9781595938350},
keywords = {best effort systems,queue,task clustering,workflow clustering},
number = {c},
pages = {8},
publisher = {ACM Press},
series = {MG '08},
title = {{Workflow task clustering for best effort systems with Pegasus}},
url = {http://portal.acm.org/citation.cfm?doid=1341811.1341822},
year = {2008}
}
